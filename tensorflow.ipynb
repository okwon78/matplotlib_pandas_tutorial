{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상수를 더하기 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ab': (3.0, 4.0), 'total': 7.0}\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(3.0, dtype=tf.float32)\n",
    "b = tf.constant(4.0)\n",
    "\n",
    "total = a + b\n",
    "\n",
    "# tensorboard를 위해 그래프 추가\n",
    "# tensorboard 실행은 tensorboard --logdir=\n",
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run({'ab':(a, b), 'total':total}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프의 동작 방식을 이해하기 위한 예제이다. 다음은 그래프가 3번 실행 된다.\n",
    "마지막 실행에서 out1, out2가 동일한 vec값을 사용했다는 점을 확인하기 바란다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#변수 3개 반환\n",
    "vec = tf.random_uniform(shape=(3,))\n",
    "out1 = vec + 1\n",
    "out2 = vec + 2\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('vec: {}'.format(sess.run(vec)))\n",
    "    print('vec: {}'.format(sess.run(vec)))\n",
    "    # out1, out2가 동일한 vec을 사용했다\n",
    "    print('out: {}'.format(sess.run((out1, out2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 값을 넣기 위해 placeholder를 추가한 내용이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "z = x + y\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(z, feed_dict={x: 3, y: 4.5}))\n",
    "    print(sess.run(z, feed_dict={x: [1, 3], y:[2, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = np.array([\n",
    "    [0, 1],\n",
    "    [2, 3],\n",
    "    [4, 5],\n",
    "    [6, 7],\n",
    "])\n",
    "\n",
    "slices = tf.data.Dataset.from_tensor_slices(my_data)\n",
    "iterator = slices.make_one_shot_iterator()\n",
    "next_item = iterator.get_next()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        print(sess.run(next_item))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_data = tf.random_normal([10, 3])\n",
    "slices = tf.data.Dataset.from_tensor_slices(my_data)\n",
    "iterator = slices.make_initializable_iterator()\n",
    "next_row = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    \n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            i+=1\n",
    "            print(i, sess.run(next_row))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 소스로 부터 dataset을 구성\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10], minval=0, maxval=100))\n",
    "print(dataset1.output_types)\n",
    "print(dataset1.output_shapes)\n",
    "\n",
    "# 다른 입력 소스를 묶어서 dataset을 구성\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.random_uniform([4]), tf.random_uniform([4, 100]))\n",
    ")\n",
    "\n",
    "print(dataset2.output_types)\n",
    "print(dataset2.output_shapes)\n",
    "\n",
    "# dataset을 묶어서 새로운 dataset을 구성\n",
    "dataset3 = tf.data.Dataset.zip(\n",
    "    (dataset1, dataset2)\n",
    ")\n",
    "print(dataset3.output_types)\n",
    "print(dataset3.output_shapes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딕셔너리를 이용한 입력소스 지정 방법. key에 이름을 부여할 수 있음\n",
    "sources = dict()\n",
    "\n",
    "sources['a'] = tf.random_uniform([4])\n",
    "sources['b'] = tf.random_uniform([4, 100])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(sources)\n",
    "\n",
    "print(dataset.output_types)\n",
    "print(dataset.output_shapes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. one-shot iterator\n",
    "* initialization이 필요 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(10):\n",
    "      print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. initalizable iterator\n",
    "* 사용전, initialization 필수\n",
    "* tf.placeholder를 이용해서 동작을 지정할 있는 장점이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = tf.placeholder(tf.int64, shape=[])\n",
    "dataset = tf.data.Dataset.range(max_value)\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer, feed_dict={max_value: 20})\n",
    "    for i in range(10):\n",
    "        print(sess.run(next_element))\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer, feed_dict={max_value:5})\n",
    "    for i in range(5):\n",
    "        print(sess.run(next_element))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. reinitilizable iterator\n",
    "* 여러 Data source로 부터 데이터를 가지고 올수 있다. 단, structure은 동일해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.range(10).map(lambda x: x + tf.random_uniform([], -10, 10, tf.int64))\n",
    "validation_dataset = tf.data.Dataset.range(5)\n",
    "\n",
    "# reinitializable iterator의 경우는 structure로 정의된다.\n",
    "# 다른 Data source라 할지라도 같은 데이터 형태를 가지기 때문에 둘 중 하나를 사용하면 된다.\n",
    "iterator = tf.data.Iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "validation_init_op = iterator.make_initializer(validation_dataset)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    #The number of epochs\n",
    "    for e in range(20):\n",
    "        print(f'> epoch {e}')\n",
    "        \n",
    "        # train sequence가 초기화 된다.\n",
    "        sess.run(training_init_op)\n",
    "        for _ in range(10):\n",
    "            print(f'train: {sess.run(next_element)}')\n",
    "        # validation sequence가 초기화 된다.\n",
    "        sess.run(validation_init_op)\n",
    "        for _ in range(5):\n",
    "            print(f'valid: {sess.run(next_element)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. feedable iterator\n",
    "* reinitializable iterator와 동일 기능 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat함수를 이용해서 반복되는 sequence를 생성할 수 있다.\n",
    "# repeat함수를 사용하지 않으면 범위를 벗어날 경우, OutOfRangeError가 발생한다.\n",
    "training_dataset = tf.data.Dataset.range(10).repeat()\n",
    "validation_dataset = tf.data.Dataset.range(5)\n",
    "\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(handle, \n",
    "                                               validation_dataset.output_types, \n",
    "                                               validation_dataset.output_shapes)\n",
    "\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "training_iterator = training_dataset.make_one_shot_iterator()\n",
    "validation_iterator = validation_dataset.make_initializable_iterator()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # input 소스를 선택할 수 있는 핸들러\n",
    "    training_handle = sess.run(training_iterator.string_handle())\n",
    "    validation_handle = sess.run(validation_iterator.string_handle())\n",
    "    \n",
    "    while True:\n",
    "        for _ in range(200):\n",
    "            print('train ', sess.run(next_element, feed_dict={handle: training_handle}))\n",
    "\n",
    "        sess.run(validation_iterator.initializer)\n",
    "        for _ in range(5):\n",
    "            print('valid ', sess.run(next_element, feed_dict={handle: validation_handle}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(3)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "result = tf.add(next_element, next_element)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "   \n",
    "    # 일반적으로 train loop에 사용하는 try-except 블록\n",
    "    try:\n",
    "        print(sess.run(result)) # 0\n",
    "        print(sess.run(result)) # 1\n",
    "        print(sess.run(result)) # 2\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('End of dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(['A', 'B', 'C'])\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(([1, 2, 3], ['A1', 'B1', 'C1']))\n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "\n",
    "iterator = dataset3.make_initializable_iterator()\n",
    "next1, (next2, next3) = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    print(sess.run((next1, (next2, next3))))\n",
    "    # iterator에 묶여 있어서 동시에 다음 element로 넘어간다.\n",
    "    print(sess.run(next1)) \n",
    "    print(sess.run((next1, (next2, next3))))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Reading input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Consuming NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "y = np.array([0, 1])\n",
    "\n",
    "np.savez('./array', features=x, labels=y)\n",
    "data = np.load('array.npz')\n",
    "\n",
    "features = data['features']\n",
    "labels = data['labels']\n",
    "\n",
    "# The length of features and labels should be the same.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "feature_placeholder = tf.placeholder(features.dtype, features.shape)\n",
    "labels_placeholder = tf.placeholder(labels.dtype, labels.shape)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((feature_placeholder, labels_placeholder))\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer, feed_dict={feature_placeholder: features, labels_placeholder: labels})\n",
    "    print(sess.run(next_element))\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Consuming TFRecord data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Consuming text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Preprocessing data with Dataset.map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Parsing tf.Example protocol buffer messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Decoding image data and resizing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Applying arbitrary Python logic with tf.py_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Batching dataset elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Simple batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inc_dataset = tf.data.Dataset.range(100)\n",
    "dec_dataset = tf.data.Dataset.range(0, -100, -1)\n",
    "dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))\n",
    "# basch 사이즈 지정\n",
    "\n",
    "batched_dataset = dataset.batch(4)\n",
    "\n",
    "iterator = batched_dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(next_element))  # ==> ([0, 1, 2,   3],   [ 0, -1,  -2,  -3])\n",
    "    print(sess.run(next_element))  # ==> ([4, 5, 6,   7],   [-4, -5,  -6,  -7])\n",
    "    print(sess.run(next_element))  # ==> ([8, 9, 10, 11],   [-8, -9, -10, -11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Batching tensors with padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
